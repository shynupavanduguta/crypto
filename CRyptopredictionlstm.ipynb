{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SlffXr_PxZzg",
        "outputId": "3f733ef7-dff7-4058-9aad-908735d7788d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "47/47 [==============================] - 13s 143ms/step - loss: 0.0178\n",
            "Epoch 2/100\n",
            "47/47 [==============================] - 5s 108ms/step - loss: 0.0014\n",
            "Epoch 3/100\n",
            "47/47 [==============================] - 6s 134ms/step - loss: 0.0012\n",
            "Epoch 4/100\n",
            "47/47 [==============================] - 5s 114ms/step - loss: 9.9233e-04\n",
            "Epoch 5/100\n",
            "47/47 [==============================] - 6s 125ms/step - loss: 9.3043e-04\n",
            "Epoch 6/100\n",
            "47/47 [==============================] - 6s 119ms/step - loss: 7.9971e-04\n",
            "Epoch 7/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 8.9913e-04\n",
            "Epoch 8/100\n",
            "47/47 [==============================] - 6s 134ms/step - loss: 7.1004e-04\n",
            "Epoch 9/100\n",
            "47/47 [==============================] - 5s 108ms/step - loss: 7.0293e-04\n",
            "Epoch 10/100\n",
            "47/47 [==============================] - 6s 134ms/step - loss: 6.1016e-04\n",
            "Epoch 11/100\n",
            "47/47 [==============================] - 5s 109ms/step - loss: 5.4050e-04\n",
            "Epoch 12/100\n",
            "47/47 [==============================] - 6s 134ms/step - loss: 5.1146e-04\n",
            "Epoch 13/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 5.4265e-04\n",
            "Epoch 14/100\n",
            "47/47 [==============================] - 5s 114ms/step - loss: 4.4578e-04\n",
            "Epoch 15/100\n",
            "47/47 [==============================] - 6s 123ms/step - loss: 4.7461e-04\n",
            "Epoch 16/100\n",
            "47/47 [==============================] - 5s 112ms/step - loss: 4.3501e-04\n",
            "Epoch 17/100\n",
            "47/47 [==============================] - 6s 136ms/step - loss: 4.8008e-04\n",
            "Epoch 18/100\n",
            "47/47 [==============================] - 5s 111ms/step - loss: 4.0472e-04\n",
            "Epoch 19/100\n",
            "47/47 [==============================] - 6s 134ms/step - loss: 4.0834e-04\n",
            "Epoch 20/100\n",
            "47/47 [==============================] - 5s 112ms/step - loss: 4.7927e-04\n",
            "Epoch 21/100\n",
            "47/47 [==============================] - 6s 133ms/step - loss: 3.4301e-04\n",
            "Epoch 22/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 4.0937e-04\n",
            "Epoch 23/100\n",
            "47/47 [==============================] - 6s 125ms/step - loss: 3.1714e-04\n",
            "Epoch 24/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 3.1758e-04\n",
            "Epoch 25/100\n",
            "47/47 [==============================] - 5s 114ms/step - loss: 2.8609e-04\n",
            "Epoch 26/100\n",
            "47/47 [==============================] - 5s 111ms/step - loss: 3.0693e-04\n",
            "Epoch 27/100\n",
            "47/47 [==============================] - 5s 108ms/step - loss: 2.7142e-04\n",
            "Epoch 28/100\n",
            "47/47 [==============================] - 6s 119ms/step - loss: 2.4818e-04\n",
            "Epoch 29/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 2.3767e-04\n",
            "Epoch 30/100\n",
            "47/47 [==============================] - 6s 119ms/step - loss: 2.6230e-04\n",
            "Epoch 31/100\n",
            "47/47 [==============================] - 5s 105ms/step - loss: 2.4878e-04\n",
            "Epoch 32/100\n",
            "47/47 [==============================] - 6s 123ms/step - loss: 2.2293e-04\n",
            "Epoch 33/100\n",
            "47/47 [==============================] - 5s 109ms/step - loss: 2.3374e-04\n",
            "Epoch 34/100\n",
            "47/47 [==============================] - 6s 119ms/step - loss: 2.4787e-04\n",
            "Epoch 35/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 2.2418e-04\n",
            "Epoch 36/100\n",
            "47/47 [==============================] - 5s 113ms/step - loss: 2.5592e-04\n",
            "Epoch 37/100\n",
            "47/47 [==============================] - 6s 120ms/step - loss: 2.6605e-04\n",
            "Epoch 38/100\n",
            "47/47 [==============================] - 5s 103ms/step - loss: 2.1868e-04\n",
            "Epoch 39/100\n",
            "47/47 [==============================] - 6s 122ms/step - loss: 2.5496e-04\n",
            "Epoch 40/100\n",
            "47/47 [==============================] - 5s 108ms/step - loss: 2.4228e-04\n",
            "Epoch 41/100\n",
            "47/47 [==============================] - 6s 124ms/step - loss: 2.3968e-04\n",
            "Epoch 42/100\n",
            "47/47 [==============================] - 5s 114ms/step - loss: 2.1796e-04\n",
            "Epoch 43/100\n",
            "47/47 [==============================] - 6s 118ms/step - loss: 2.0753e-04\n",
            "Epoch 44/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 2.5495e-04\n",
            "Epoch 45/100\n",
            "47/47 [==============================] - 6s 127ms/step - loss: 2.6651e-04\n",
            "Epoch 46/100\n",
            "47/47 [==============================] - 5s 106ms/step - loss: 1.9180e-04\n",
            "Epoch 47/100\n",
            "47/47 [==============================] - 6s 125ms/step - loss: 1.8562e-04\n",
            "Epoch 48/100\n",
            "47/47 [==============================] - 5s 109ms/step - loss: 1.8784e-04\n",
            "Epoch 49/100\n",
            "47/47 [==============================] - 5s 114ms/step - loss: 1.8211e-04\n",
            "Epoch 50/100\n",
            "47/47 [==============================] - 5s 113ms/step - loss: 1.9810e-04\n",
            "Epoch 51/100\n",
            "47/47 [==============================] - 5s 107ms/step - loss: 1.9177e-04\n",
            "Epoch 52/100\n",
            "47/47 [==============================] - 6s 126ms/step - loss: 1.7467e-04\n",
            "Epoch 53/100\n",
            "47/47 [==============================] - 5s 107ms/step - loss: 1.7058e-04\n",
            "Epoch 54/100\n",
            "47/47 [==============================] - 6s 126ms/step - loss: 1.7514e-04\n",
            "Epoch 55/100\n",
            "47/47 [==============================] - 5s 107ms/step - loss: 1.7308e-04\n",
            "Epoch 56/100\n",
            "47/47 [==============================] - 6s 124ms/step - loss: 1.8264e-04\n",
            "Epoch 57/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 1.8351e-04\n",
            "Epoch 58/100\n",
            "47/47 [==============================] - 6s 128ms/step - loss: 1.9763e-04\n",
            "Epoch 59/100\n",
            "47/47 [==============================] - 5s 103ms/step - loss: 2.1480e-04\n",
            "Epoch 60/100\n",
            "47/47 [==============================] - 5s 115ms/step - loss: 2.2835e-04\n",
            "Epoch 61/100\n",
            "47/47 [==============================] - 5s 111ms/step - loss: 1.9898e-04\n",
            "Epoch 62/100\n",
            "47/47 [==============================] - 5s 113ms/step - loss: 1.9436e-04\n",
            "Epoch 63/100\n",
            "47/47 [==============================] - 6s 117ms/step - loss: 1.7877e-04\n",
            "Epoch 64/100\n",
            "47/47 [==============================] - 5s 100ms/step - loss: 1.6659e-04\n",
            "Epoch 65/100\n",
            "47/47 [==============================] - 6s 121ms/step - loss: 1.9892e-04\n",
            "Epoch 66/100\n",
            "47/47 [==============================] - 5s 111ms/step - loss: 1.9345e-04\n",
            "Epoch 67/100\n",
            "47/47 [==============================] - 6s 124ms/step - loss: 1.8985e-04\n",
            "Epoch 68/100\n",
            "47/47 [==============================] - 5s 107ms/step - loss: 1.7743e-04\n",
            "Epoch 69/100\n",
            "47/47 [==============================] - 6s 125ms/step - loss: 1.8188e-04\n",
            "Epoch 70/100\n",
            "47/47 [==============================] - 5s 106ms/step - loss: 1.8755e-04\n",
            "Epoch 71/100\n",
            "47/47 [==============================] - 6s 126ms/step - loss: 1.9094e-04\n",
            "Epoch 72/100\n",
            "47/47 [==============================] - 5s 109ms/step - loss: 1.9851e-04\n",
            "Epoch 73/100\n",
            "47/47 [==============================] - 6s 120ms/step - loss: 1.9004e-04\n",
            "Epoch 74/100\n",
            "47/47 [==============================] - 5s 111ms/step - loss: 1.8211e-04\n",
            "Epoch 75/100\n",
            "47/47 [==============================] - 5s 109ms/step - loss: 1.7870e-04\n",
            "Epoch 76/100\n",
            "47/47 [==============================] - 6s 123ms/step - loss: 2.0366e-04\n",
            "Epoch 77/100\n",
            "47/47 [==============================] - 5s 108ms/step - loss: 1.7255e-04\n",
            "Epoch 78/100\n",
            "47/47 [==============================] - 6s 127ms/step - loss: 1.7335e-04\n",
            "Epoch 79/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 1.8529e-04\n",
            "Epoch 80/100\n",
            "47/47 [==============================] - 6s 125ms/step - loss: 1.9630e-04\n",
            "Epoch 81/100\n",
            "47/47 [==============================] - 5s 108ms/step - loss: 1.6353e-04\n",
            "Epoch 82/100\n",
            "47/47 [==============================] - 6s 126ms/step - loss: 2.0574e-04\n",
            "Epoch 83/100\n",
            "47/47 [==============================] - 5s 112ms/step - loss: 1.7444e-04\n",
            "Epoch 84/100\n",
            "47/47 [==============================] - 6s 120ms/step - loss: 1.7499e-04\n",
            "Epoch 85/100\n",
            "47/47 [==============================] - 5s 102ms/step - loss: 1.8263e-04\n",
            "Epoch 86/100\n",
            "47/47 [==============================] - 5s 115ms/step - loss: 1.9232e-04\n",
            "Epoch 87/100\n",
            "47/47 [==============================] - 5s 113ms/step - loss: 1.7917e-04\n",
            "Epoch 88/100\n",
            "47/47 [==============================] - 5s 106ms/step - loss: 1.7000e-04\n",
            "Epoch 89/100\n",
            "47/47 [==============================] - 6s 122ms/step - loss: 1.7309e-04\n",
            "Epoch 90/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 1.9261e-04\n",
            "Epoch 91/100\n",
            "47/47 [==============================] - 6s 126ms/step - loss: 1.9616e-04\n",
            "Epoch 92/100\n",
            "47/47 [==============================] - 5s 109ms/step - loss: 1.7337e-04\n",
            "Epoch 93/100\n",
            "47/47 [==============================] - 6s 121ms/step - loss: 1.6664e-04\n",
            "Epoch 94/100\n",
            "47/47 [==============================] - 5s 103ms/step - loss: 1.5753e-04\n",
            "Epoch 95/100\n",
            "47/47 [==============================] - 6s 124ms/step - loss: 1.6762e-04\n",
            "Epoch 96/100\n",
            "47/47 [==============================] - 5s 109ms/step - loss: 1.6311e-04\n",
            "Epoch 97/100\n",
            "47/47 [==============================] - 6s 123ms/step - loss: 1.6438e-04\n",
            "Epoch 98/100\n",
            "47/47 [==============================] - 5s 110ms/step - loss: 1.7697e-04\n",
            "Epoch 99/100\n",
            "47/47 [==============================] - 5s 115ms/step - loss: 2.1009e-04\n",
            "Epoch 100/100\n",
            "47/47 [==============================] - 6s 116ms/step - loss: 1.7007e-04\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "prediction 1: Predicted Price: 27690.140625\n",
            "prediction 2: Predicted Price: 27739.359375\n",
            "prediction 3: Predicted Price: 27837.525390625\n",
            "prediction 4: Predicted Price: 27953.220703125\n",
            "prediction 5: Predicted Price: 28071.337890625\n",
            "prediction 6: Predicted Price: 28183.33203125\n",
            "prediction 7: Predicted Price: 28286.525390625\n",
            "prediction 8: Predicted Price: 28382.466796875\n",
            "prediction 9: Predicted Price: 28474.912109375\n",
            "prediction 10: Predicted Price: 28568.12890625\n",
            "prediction 11: Predicted Price: 28665.63671875\n",
            "prediction 12: Predicted Price: 28769.52734375\n",
            "prediction 13: Predicted Price: 28880.330078125\n",
            "prediction 14: Predicted Price: 28997.259765625\n",
            "prediction 15: Predicted Price: 29118.53125\n",
            "prediction 16: Predicted Price: 29241.947265625\n",
            "prediction 17: Predicted Price: 29365.220703125\n",
            "prediction 18: Predicted Price: 29486.357421875\n",
            "prediction 19: Predicted Price: 29603.8515625\n",
            "prediction 20: Predicted Price: 29716.734375\n",
            "prediction 21: Predicted Price: 29824.552734375\n",
            "prediction 22: Predicted Price: 29927.294921875\n",
            "prediction 23: Predicted Price: 30025.24609375\n",
            "prediction 24: Predicted Price: 30118.93359375\n",
            "prediction 25: Predicted Price: 30208.9921875\n",
            "prediction 26: Predicted Price: 30296.064453125\n",
            "prediction 27: Predicted Price: 30380.80078125\n",
            "prediction 28: Predicted Price: 30463.8046875\n",
            "prediction 29: Predicted Price: 30545.62109375\n",
            "prediction 30: Predicted Price: 30626.767578125\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "\n",
        "data = pd.read_csv('/.csv/bitcoin_csv')\n",
        "\n",
        "# Preprocess the data\n",
        "prices = data['Close'].values.reshape(-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_prices = scaler.fit_transform(prices)\n",
        "\n",
        "# Define a function to create input features and target values\n",
        "def create_dataset(prices, time_step=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(prices)-time_step-1):\n",
        "        a = prices[i:(i+time_step), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(prices[i + time_step, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "time_step = 60\n",
        "\n",
        "# Create the input features and target values\n",
        "trainX, trainY = create_dataset(scaled_prices, time_step)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64)\n",
        "\n",
        "\n",
        "last_month_prices = scaled_prices[-time_step:, :]\n",
        "next_month_predictions = []\n",
        "\n",
        "for _ in range(30):\n",
        "    input_data = np.expand_dims(last_month_prices, axis=0)\n",
        "    predicted_price = model.predict(input_data)[0, 0]\n",
        "    next_month_predictions.append(predicted_price)\n",
        "\n",
        "    # Shift the input data by one day and update the last month prices\n",
        "    last_month_prices = np.roll(last_month_prices, -1)\n",
        "    last_month_prices[-1] = predicted_price\n",
        "\n",
        "# Inverse scale the predictions\n",
        "next_month_predictions = np.array(next_month_predictions).reshape(-1, 1)\n",
        "next_month_predictions = scaler.inverse_transform(next_month_predictions)\n",
        "\n",
        "# Print the predicted prices for the next month\n",
        "for i, price in enumerate(next_month_predictions, 1):\n",
        "    print(f\"prediction {i}: Predicted Price: {price[0]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}